{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff14bf4-cec5-4ddc-8104-5409d475259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dust3r')\n",
    "sys.path.append('../gaussian-splatting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe15815-ee0a-4e75-a1ed-d58e10e24758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/wildgaussians/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from dust3r.inference import inference, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.utils.device import to_numpy\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import lovely_tensors as lt\n",
    "except:\n",
    "    ! pip install --upgrade lovely-tensors\n",
    "    import lovely_tensors as lt\n",
    "    \n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93933eb0-8885-4511-a8de-1071bdf2a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554b5195-0a67-401a-8817-a0e5b37e11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path.ls = lambda x: list(x.iterdir())\n",
    "\n",
    "image_dir = Path('../data/images/turtle_imgs/')\n",
    "\n",
    "image_files = [str(x) for x in image_dir.ls() if x.suffix in ['.png', '.jpg']]\n",
    "image_files = sorted(image_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc8d0fd-3aa4-41f2-ba88-3f257173d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from ../dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n",
      ">> Loading a list of 3 images\n",
      " - adding ../data/images/turtle_imgs/1.jpg with resolution 964x1280 --> 384x512\n",
      " - adding ../data/images/turtle_imgs/2.jpg with resolution 964x1280 --> 384x512\n",
      " - adding ../data/images/turtle_imgs/3.jpg with resolution 964x1280 --> 384x512\n",
      " (Found 3 images)\n",
      ">> Inference with model on 6 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_path, device)\n",
    "images = load_images(image_files, size=512)\n",
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6595b9e7-3eb0-4a44-9505-bb97b3bb488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (1*,0*) score=21.04109764099121\n",
      " init edge (2*,1) score=15.99399471282959\n",
      " init loss = 0.00863318145275116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:06<00:00, 48.03it/s, lr=1.27413e-06 loss=0.00331271]\n"
     ]
    }
   ],
   "source": [
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6184d-151c-4b6d-9910-6d497f57d44c",
   "metadata": {},
   "source": [
    "# Construct colmap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec50521-d426-4151-9bd7-c1f72454bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(mat):\n",
    "    \"\"\" Invert a torch or numpy matrix\n",
    "    \"\"\"\n",
    "    if isinstance(mat, torch.Tensor):\n",
    "        return torch.linalg.inv(mat)\n",
    "    if isinstance(mat, np.ndarray):\n",
    "        return np.linalg.inv(mat)\n",
    "    raise ValueError(f'bad matrix type = {type(mat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342a4c9c-418b-4070-9a52-5b0f022aab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics = scene.get_intrinsics().detach().cpu().numpy()\n",
    "world2cam = inv(scene.get_im_poses().detach()).cpu().numpy()\n",
    "principal_points = scene.get_principal_points().detach().cpu().numpy()\n",
    "focals = scene.get_focals().detach().cpu().numpy()\n",
    "imgs = np.array(scene.imgs)\n",
    "pts3d = [i.detach() for i in scene.get_pts3d()]\n",
    "depth_maps = [i.detach() for i in scene.get_depthmaps()]\n",
    "\n",
    "min_conf_thr = 20\n",
    "scene.min_conf_thr = float(scene.conf_trf(torch.tensor(min_conf_thr)))\n",
    "masks = to_numpy(scene.get_masks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d82908-d722-4bd8-be6e-55a88798481e",
   "metadata": {},
   "source": [
    "After convertion such data sctructure should appear\n",
    "\n",
    "```\n",
    "│   │   │   ├── images\n",
    "│   │   │   ├── masks\n",
    "│   │   │   ├── sparse/0\n",
    "|   |   |   |    |------cameras.bin\n",
    "|   |   |   |    |------images.bin\n",
    "|   |   |   |    |------points3D.bin\n",
    "|   |   |   |    |------points3D.ply\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d54baea-2a00-4872-a64b-be90872907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('../data/scenes/turtle')\n",
    "save_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6beff5a-5b8f-4130-b572-09ba73768f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple, Optional\n",
    "import cv2  # Assuming OpenCV is used for image saving\n",
    "from scene.gaussian_model import BasicPointCloud\n",
    "from PIL import Image\n",
    "from scene.colmap_loader import rotmat2qvec\n",
    "from utils.graphics_utils import focal2fov, fov2focal\n",
    "from scene.dataset_readers import storePly\n",
    "\n",
    "class CameraInfo(NamedTuple):\n",
    "    uid: int\n",
    "    R: np.ndarray\n",
    "    T: np.ndarray\n",
    "    FovY: np.ndarray\n",
    "    FovX: np.ndarray\n",
    "    image: np.ndarray\n",
    "    image_path: str\n",
    "    image_name: str\n",
    "    width: int\n",
    "    height: int\n",
    "    mask: Optional[np.ndarray] = None\n",
    "    mono_depth: Optional[np.ndarray] = None\n",
    "\n",
    "class SceneInfo(NamedTuple):\n",
    "    point_cloud: BasicPointCloud\n",
    "    train_cameras: list\n",
    "    test_cameras: list\n",
    "    nerf_normalization: dict\n",
    "    ply_path: str\n",
    "    render_cameras: Optional[list[CameraInfo]] = None\n",
    "    \n",
    "def init_filestructure(save_path):\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    images_path = save_path / 'images'\n",
    "    masks_path = save_path / 'masks'\n",
    "    sparse_path = save_path / 'sparse/0'\n",
    "    \n",
    "    images_path.mkdir(exist_ok=True, parents=True)\n",
    "    masks_path.mkdir(exist_ok=True, parents=True)    \n",
    "    sparse_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    return save_path, images_path, masks_path, sparse_path\n",
    "\n",
    "def save_images_masks(imgs, masks, images_path, masks_path):\n",
    "    # Saving images and optionally masks/depth maps\n",
    "    for i, (image, mask) in enumerate(zip(imgs, masks)):\n",
    "        image_save_path = images_path / f\"{i}.png\"\n",
    "        \n",
    "        mask_save_path = masks_path / f\"{i}.png\"\n",
    "        image[~mask] = 1.\n",
    "        rgb_image = cv2.cvtColor(image*255, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(str(image_save_path), rgb_image)\n",
    "        \n",
    "        mask = np.repeat(np.expand_dims(mask, -1), 3, axis=2)*255\n",
    "        Image.fromarray(mask.astype(np.uint8)).save(mask_save_path)\n",
    "        \n",
    "        \n",
    "def save_cameras(focals, principal_points, sparse_path, imgs_shape):\n",
    "    # Save cameras.txt\n",
    "    cameras_file = sparse_path / 'cameras.txt'\n",
    "    with open(cameras_file, 'w') as cameras_file:\n",
    "        cameras_file.write(\"# Camera list with one line of data per camera:\\n\")\n",
    "        cameras_file.write(\"# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\")\n",
    "        for i, (focal, pp) in enumerate(zip(focals, principal_points)):\n",
    "            cameras_file.write(f\"{i} PINHOLE {imgs_shape[2]} {imgs_shape[1]} {focal[0]} {focal[0]} {pp[0]} {pp[1]}\\n\")\n",
    "            \n",
    "def save_imagestxt(world2cam, sparse_path):\n",
    "     # Save images.txt\n",
    "    images_file = sparse_path / 'images.txt'\n",
    "    # Generate images.txt content\n",
    "    with open(images_file, 'w') as images_file:\n",
    "        images_file.write(\"# Image list with two lines of data per image:\\n\")\n",
    "        images_file.write(\"# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\\n\")\n",
    "        images_file.write(\"# POINTS2D[] as (X, Y, POINT3D_ID)\\n\")\n",
    "        for i in range(world2cam.shape[0]):\n",
    "            # Convert rotation matrix to quaternion\n",
    "            rotation_matrix = world2cam[i, :3, :3]\n",
    "            qw, qx, qy, qz = rotmat2qvec(rotation_matrix)\n",
    "            tx, ty, tz = world2cam[i, :3, 3]\n",
    "            images_file.write(f\"{i} {qw} {qx} {qy} {qz} {tx} {ty} {tz} {i} {i}.png\\n\")\n",
    "            images_file.write(\"\\n\") # Placeholder for points, assuming no points are associated with images here\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_pointcloud_with_normals(imgs, pts3d, msk, sparse_path):\n",
    "    pc = get_pc(imgs, pts3d, msk)  # Assuming get_pc is defined elsewhere and returns a trimesh point cloud\n",
    "\n",
    "    # Define a default normal, e.g., [0, 1, 0]\n",
    "    default_normal = [0, 1, 0]\n",
    "\n",
    "    # Prepare vertices, colors, and normals for saving\n",
    "    vertices = pc.vertices\n",
    "    colors = pc.colors\n",
    "    normals = np.tile(default_normal, (vertices.shape[0], 1))\n",
    "\n",
    "    save_path = sparse_path / 'points3D.ply'\n",
    "\n",
    "    # Construct the header of the PLY file\n",
    "    header = \"\"\"ply\n",
    "format ascii 1.0\n",
    "element vertex {}\n",
    "property float x\n",
    "property float y\n",
    "property float z\n",
    "property uchar red\n",
    "property uchar green\n",
    "property uchar blue\n",
    "property float nx\n",
    "property float ny\n",
    "property float nz\n",
    "end_header\n",
    "\"\"\".format(len(vertices))\n",
    "\n",
    "    # Write the PLY file\n",
    "    with open(save_path, 'w') as ply_file:\n",
    "        ply_file.write(header)\n",
    "        for vertex, color, normal in zip(vertices, colors, normals):\n",
    "            ply_file.write('{} {} {} {} {} {} {} {} {}\\n'.format(\n",
    "                vertex[0], vertex[1], vertex[2],\n",
    "                int(color[0]), int(color[1]), int(color[2]),\n",
    "                normal[0], normal[1], normal[2]\n",
    "            ))\n",
    "            \n",
    "import trimesh\n",
    "def get_pc(imgs, pts3d, mask):\n",
    "    imgs = to_numpy(imgs)\n",
    "    pts3d = to_numpy(pts3d)\n",
    "    mask = to_numpy(mask)\n",
    "    \n",
    "    pts = np.concatenate([p[m] for p, m in zip(pts3d, mask)])\n",
    "    col = np.concatenate([p[m] for p, m in zip(imgs, mask)])\n",
    "    \n",
    "    pts = pts.reshape(-1, 3)[::3]\n",
    "    col = col.reshape(-1, 3)[::3]\n",
    "    \n",
    "    #mock normals:\n",
    "    normals = np.tile([0, 1, 0], (pts.shape[0], 1))\n",
    "    \n",
    "    pct = trimesh.PointCloud(pts, colors=col)\n",
    "    pct.vertices_normal = normals  # Manually add normals to the point cloud\n",
    "    \n",
    "    return pct#, pts\n",
    "\n",
    "def save_pointcloud(imgs, pts3d, msk, sparse_path):\n",
    "    save_path = sparse_path / 'points3D.ply'\n",
    "    pc = get_pc(imgs, pts3d, msk)\n",
    "    \n",
    "    pc.export(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c888ccf6-c8f7-4a15-a129-61be14ce3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path, images_path, masks_path, sparse_path = init_filestructure(save_dir)\n",
    "save_images_masks(imgs, masks, images_path, masks_path)\n",
    "save_cameras(focals, principal_points, sparse_path, imgs_shape=imgs.shape)\n",
    "save_imagestxt(world2cam, sparse_path)\n",
    "# save_pointcloud(imgs, pts3d, masks, sparse_path)\n",
    "save_pointcloud_with_normals(imgs, pts3d, masks, sparse_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
